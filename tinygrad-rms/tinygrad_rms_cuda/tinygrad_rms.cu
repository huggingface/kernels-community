#include <ATen/cuda/CUDAContext.h>
#include <c10/cuda/CUDAGuard.h>
#include <torch/all.h>

// RMSNorm CUDA kernels generated by tinygrad
// These kernels compute: output = input * (1 / sqrt(mean(input^2) + epsilon))
// Shape: (batch * seq_len, hidden_size) where hidden_size = 1024

// Kernel: r_N_4_4_8_128
// Computes 1/sqrt(mean(x^2) + eps) for each row
// Grid: (num_rows / 16), Block: (4, 8) = 32 threads
__global__ void __launch_bounds__(32) r_N_4_4_8_128(
    float* data0,       // rms_inv output [num_rows]
    float* data1,       // input [num_rows * 1024]
    int num_rows) {
  __shared__ __align__(16) float temp0[128];
  float acc0[4];
  float acc1[4];
  int gidx0 = blockIdx.x;
  int lidx0 = threadIdx.x; /* 4 */
  int lidx1 = threadIdx.y; /* 8 */
  int alu0 = (lidx0 << 5);
  *(acc1 + 0) = 0.0f;
  *(acc1 + 1) = 0.0f;
  *(acc1 + 2) = 0.0f;
  *(acc1 + 3) = 0.0f;
  int alu5 = (lidx0 + (gidx0 << 4));
  *(acc0 + 0) = 0.0f;
  *(acc0 + 1) = 0.0f;
  *(acc0 + 2) = 0.0f;
  *(acc0 + 3) = 0.0f;
  for (int ridx3004 = 0; ridx3004 < 128; ridx3004++) {
    int alu10 = (lidx1 + (gidx0 << 14) + (lidx0 << 10) + (ridx3004 << 3));
    float val0 = (*(data1 + alu10));
    float val1 = (*(data1 + (alu10 + 4096)));
    float val2 = (*(data1 + (alu10 + 8192)));
    float val3 = (*(data1 + (alu10 + 12288)));
    *(acc0 + 0) = ((*(acc0 + 0)) + (val0 * val0));
    *(acc0 + 1) = ((*(acc0 + 1)) + (val1 * val1));
    *(acc0 + 2) = ((*(acc0 + 2)) + (val2 * val2));
    *(acc0 + 3) = ((*(acc0 + 3)) + (val3 * val3));
  }
  *((float4*)((temp0 + (alu0 + (lidx1 << 2))))) =
      make_float4((*(acc0 + 0)), (*(acc0 + 1)), (*(acc0 + 2)), (*(acc0 + 3)));
  __syncthreads();
  if ((((bool)(lidx1)) != 1)) {
    for (int ridx1003 = 0; ridx1003 < 8; ridx1003++) {
      float4 val4 = (*((float4*)((temp0 + (alu0 + (ridx1003 << 2))))));
      *(acc1 + 0) = ((*(acc1 + 0)) + val4.x);
      *(acc1 + 1) = ((*(acc1 + 1)) + val4.y);
      *(acc1 + 2) = ((*(acc1 + 2)) + val4.z);
      *(acc1 + 3) = ((*(acc1 + 3)) + val4.w);
    }
    *(data0 + alu5) = (1 / sqrt((((*(acc1 + 0)) * 0.0009765625f) + 1e-06f)));
    *(data0 + (alu5 + 4)) = (1 / sqrt((((*(acc1 + 1)) * 0.0009765625f) + 1e-06f)));
    *(data0 + (alu5 + 8)) = (1 / sqrt((((*(acc1 + 2)) * 0.0009765625f) + 1e-06f)));
    *(data0 + (alu5 + 12)) = (1 / sqrt((((*(acc1 + 3)) * 0.0009765625f) + 1e-06f)));
  }
}

// Kernel: E_N_16_8_16_4
// Applies normalization: output = input * rms_inv
// Grid: (16, num_rows / 8), Block: (8, 16) = 128 threads
__global__ void __launch_bounds__(128) E_N_16_8_16_4(
    float* data0,       // output [num_rows * 1024]
    float* data1,       // input [num_rows * 1024]
    float* data2) {     // rms_inv [num_rows]
  int gidx0 = blockIdx.x;  /* 16 */
  int gidx1 = blockIdx.y;  /* num_rows / 8 */
  int lidx0 = threadIdx.x; /* 8 */
  int lidx1 = threadIdx.y; /* 16 */
  float val0 = (*(data2 + (lidx0 + (gidx1 << 3))));
  int alu0 = ((gidx0 << 6) + (gidx1 << 13) + (lidx0 << 10) + (lidx1 << 2));
  float4 val1 = (*((float4*)((data1 + alu0))));
  *((float4*)((data0 + alu0))) =
      make_float4((val1.x * val0), (val1.y * val0), (val1.z * val0), (val1.w * val0));
}

void tinygrad_rms_norm(
    torch::Tensor& output,
    torch::Tensor& rms_inv,
    const torch::Tensor& input,
    double epsilon) {
  TORCH_CHECK(input.device().is_cuda(), "input must be a CUDA tensor");
  TORCH_CHECK(input.is_contiguous(), "input must be contiguous");
  TORCH_CHECK(input.scalar_type() == at::ScalarType::Float, "input must be float32");

  const int hidden_size = input.size(-1);
  const int64_t num_rows = input.numel() / hidden_size;

  // Tinygrad kernel is optimized for hidden_size=1024
  TORCH_CHECK(hidden_size == 1024,
              "tinygrad kernel requires hidden_size=1024, got ", hidden_size);
  TORCH_CHECK(num_rows % 16 == 0,
              "num_rows must be divisible by 16, got ", num_rows);

  TORCH_CHECK(output.sizes() == input.sizes(),
              "output and input must have the same shape");
  TORCH_CHECK(rms_inv.numel() == num_rows,
              "rms_inv must have ", num_rows, " elements, got ", rms_inv.numel());

  const at::cuda::OptionalCUDAGuard device_guard(device_of(input));
  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();

  // Kernel 1: compute rms_inv
  dim3 grid1(num_rows / 16);
  dim3 block1(4, 8);
  r_N_4_4_8_128<<<grid1, block1, 0, stream>>>(
      rms_inv.data_ptr<float>(),
      input.data_ptr<float>(),
      num_rows);

  // Kernel 2: apply normalization
  dim3 grid2(16, num_rows / 8);
  dim3 block2(8, 16);
  E_N_16_8_16_4<<<grid2, block2, 0, stream>>>(
      output.data_ptr<float>(),
      input.data_ptr<float>(),
      rms_inv.data_ptr<float>());
}

void tinygrad_rms_norm_inplace(
    torch::Tensor& output,
    const torch::Tensor& input,
    double epsilon) {
  const int hidden_size = input.size(-1);
  const int64_t num_rows = input.numel() / hidden_size;

  auto rms_inv = torch::empty({num_rows}, input.options());
  tinygrad_rms_norm(output, rms_inv, input, epsilon);
}
