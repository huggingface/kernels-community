# /// script
# dependencies = [
#   "numpy",
#   "tinygrad"
# ]
# ///
"""
Generate RMSNorm CUDA kernels using tinygrad's compiler.

This script generates the CUDA kernel source code that is used in this kernel package.
The generated kernels are optimized for:
- hidden_size = 1024
- num_rows divisible by 16
- float32 dtype

Usage:
    uv run scripts/generate_kernel.py
"""
import numpy as np
from tinygrad import Tensor, Device
from tinygrad.engine.realize import CompiledRunner, lower_schedule

Device.DEFAULT = "NV"


class TinygradRMSNorm:
    """Tinygrad implementation of RMSNorm."""

    def __init__(self, dim: int, eps: float = 1e-6):
        self.eps = eps
        self.weight = Tensor.ones(dim)

    def __call__(self, x: Tensor) -> Tensor:
        rms = (x**2).mean(axis=-1, keepdim=True).add(self.eps).sqrt()
        return x / rms * self.weight


if __name__ == "__main__":
    np.random.seed(42)

    batch_size, seq_len, dim = 32, 512, 1024
    input_np = np.random.randn(batch_size, seq_len, dim).astype(np.float32)

    tinygrad_input = Tensor(input_np)
    tinygrad_rmsnorm = TinygradRMSNorm(dim)

    output = tinygrad_rmsnorm(tinygrad_input)

    # Get schedule and lower to get CUDA source
    schedule, _ = output.schedule_with_vars()

    cuda_code = []
    cuda_code.append("// RMSNorm CUDA kernels generated by tinygrad\n")

    for si, ei in lower_schedule(schedule):
        if isinstance(ei.prg, CompiledRunner) and ei.prg.p.src:
            cuda_code.append(f"// Kernel: {ei.prg.p.function_name}")
            cuda_code.append(f"// {ei.metadata}")
            cuda_code.append(ei.prg.p.src)
            cuda_code.append("")

    with open("rmsnorm_kernels.cu", "w") as f:
        f.write("\n".join(cuda_code))

    print("Saved CUDA kernels to rmsnorm_kernels.cu")
    print("\nKernel source:")
    print("\n".join(cuda_code))
