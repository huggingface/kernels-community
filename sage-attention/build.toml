[general]
name = "sage-attention"
backends = ["cuda"]
version = 1

[general.hub]
repo-id = "kernels-community/sage-attention"

[torch]
src = [
    "torch-ext/torch_binding.cpp",
    "torch-ext/torch_binding.h",
]

[kernel._qattn_sm89]
backend = "cuda"
cuda-capabilities = ["8.9"]
cuda-flags = [
    "-O3",
    "-std=c++17",
    "-U__CUDA_NO_HALF_OPERATORS__",
    "-U__CUDA_NO_HALF_CONVERSIONS__",
    "--use_fast_math",
    "--threads=1",
    "-Xptxas=-v",
    "-diag-suppress=174",
]
cuda-minver = "12.6"
cxx-flags = [
    "-g",
    "-O3",
    "-fopenmp",
    "-lgomp",
    "-std=c++17",
    "-DENABLE_BF16",
]
depends = ["torch"]
include = ["."]
src = [
    "sage_attention/qattn/sm89_qk_int8_sv_f8_accum_f32_attn_inst_buf.cu",
    "sage_attention/qattn/sm89_qk_int8_sv_f8_accum_f16_attn_inst_buf.cu",
    "sage_attention/qattn/sm89_qk_int8_sv_f8_accum_f32_attn.cu",
    "sage_attention/qattn/sm89_qk_int8_sv_f8_accum_f32_fuse_v_scale_fuse_v_mean_attn.cu",
    "sage_attention/qattn/sm89_qk_int8_sv_f8_accum_f32_fuse_v_scale_attn.cu",
    "sage_attention/qattn/sm89_qk_int8_sv_f8_accum_f32_fuse_v_scale_attn_inst_buf.cu",
    "sage_attention/qattn/sm89_qk_int8_sv_f8_accum_f16_fuse_v_scale_attn_inst_buf.cu",
    "sage_attention/qattn/attn_cuda_sm89.h",
    "sage_attention/qattn/qk_int_sv_f8_cuda_sm89.cuh",
    "sage_attention/qattn/attn_utils.cuh",
]

[kernel._qattn_sm90]
backend = "cuda"
cuda-capabilities = ["9.0a"]
cuda-flags = [
    "-O3",
    "-std=c++17",
    "-U__CUDA_NO_HALF_OPERATORS__",
    "-U__CUDA_NO_HALF_CONVERSIONS__",
    "--use_fast_math",
    "--threads=1",
    "-Xptxas=-v",
    "-diag-suppress=174",
]
cuda-minver = "12.6"
cxx-flags = [
    "-g",
    "-O3",
    "-fopenmp",
    "-lgomp",
    "-std=c++17",
    "-DENABLE_BF16",
]
depends = ["torch"]
include = ["."]
src = [
    "sage_attention/qattn/qk_int_sv_f8_cuda_sm90.cu",
    "sage_attention/qattn/attn_cuda_sm90.h",
    "sage_attention/qattn/attn_utils.cuh",
]

[kernel._qattn]
backend = "cuda"
cuda-capabilities = [
    "8.0",
    "8.9",
    "9.0a",
]
cuda-flags = [
    "-O3",
    "-std=c++17",
    "-U__CUDA_NO_HALF_OPERATORS__",
    "-U__CUDA_NO_HALF_CONVERSIONS__",
    "--use_fast_math",
    "--threads=1",
    "-Xptxas=-v",
    "-diag-suppress=174",
]
cuda-minver = "12.6"
cxx-flags = [
    "-g",
    "-O3",
    "-fopenmp",
    "-lgomp",
    "-std=c++17",
    "-DENABLE_BF16",
]
depends = ["torch"]
src = [
    "sage_attention/cp_async.cuh",
    "sage_attention/dispatch_utils.h",
    "sage_attention/math.cuh",
    "sage_attention/mma.cuh",
    "sage_attention/numeric_conversion.cuh",
    "sage_attention/permuted_smem.cuh",
    "sage_attention/reduction_utils.cuh",
    "sage_attention/wgmma.cuh",
    "sage_attention/utils.cuh",
    "sage_attention/cuda_tensormap_shim.cuh",
]

[kernel._qattn_sm80]
backend = "cuda"
cuda-capabilities = ["8.0"]
cuda-flags = [
    "-O3",
    "-std=c++17",
    "-U__CUDA_NO_HALF_OPERATORS__",
    "-U__CUDA_NO_HALF_CONVERSIONS__",
    "--use_fast_math",
    "--threads=1",
    "-Xptxas=-v",
    "-diag-suppress=174",
]
cuda-minver = "12.6"
cxx-flags = [
    "-g",
    "-O3",
    "-fopenmp",
    "-lgomp",
    "-std=c++17",
    "-DENABLE_BF16",
]
depends = ["torch"]
include = ["."]
src = [
    "sage_attention/qattn/qk_int_sv_f16_cuda_sm80.cu",
    "sage_attention/qattn/attn_cuda_sm80.h",
    "sage_attention/qattn/attn_utils.cuh",
]

[kernel._fused]
backend = "cuda"
cuda-capabilities = [
    "8.0",
    "8.9",
    "9.0a",
]
cuda-flags = [
    "-O3",
    "-std=c++17",
    "-U__CUDA_NO_HALF_OPERATORS__",
    "-U__CUDA_NO_HALF_CONVERSIONS__",
    "--use_fast_math",
    "--threads=1",
    "-Xptxas=-v",
    "-diag-suppress=174",
]
cuda-minver = "12.6"
cxx-flags = [
    "-g",
    "-O3",
    "-fopenmp",
    "-lgomp",
    "-std=c++17",
    "-DENABLE_BF16",
]
depends = ["torch"]
include = ["."]
src = [
    "sage_attention/fused/fused.cu",
    "sage_attention/fused/fused.h",
]

# SM100 (Blackwell) FP4 attention kernel
[kernel._fp4attn_sm100]
backend = "cuda"
cuda-capabilities = ["10.0a"]
cuda-minver = "12.8"
cuda-flags = [
    "-O3",
    "-std=c++17",
    "-U__CUDA_NO_HALF_OPERATORS__",
    "-U__CUDA_NO_HALF_CONVERSIONS__",
    "-U__CUDA_NO_BFLOAT16_OPERATORS__",
    "-U__CUDA_NO_BFLOAT16_CONVERSIONS__",
    "-U__CUDA_NO_BFLOAT162_OPERATORS__",
    "-U__CUDA_NO_BFLOAT162_CONVERSIONS__",
    "--expt-relaxed-constexpr",
    "--expt-extended-lambda",
    "--use_fast_math",
    "--ptxas-options=--verbose,--warn-on-local-memory-usage",
    "-lineinfo",
    "-DCUTLASS_DEBUG_TRACE_LEVEL=0",
    "-DNDEBUG",
    "-DQBLKSIZE=128",
    "-DKBLKSIZE=128",
    "-DCTA256",
    "-DDQINRMEM",
    "-DEXECMODE=0",
]
cxx-flags = [
    "-O3",
    "-std=c++17",
]
depends = [
    "torch",
    "cutlass_4_0",
]
include = [
    "sage_attention/blackwell",
]
src = [
    "sage_attention/blackwell/api.cu",
    "sage_attention/blackwell/block_info.h",
    "sage_attention/blackwell/blockscaled_layout.h",
    "sage_attention/blackwell/cute_extension.h",
    "sage_attention/blackwell/epilogue_tma_ws.h",
    "sage_attention/blackwell/kernel_traits.h",
    "sage_attention/blackwell/kernel_ws.h",
    "sage_attention/blackwell/launch.h",
    "sage_attention/blackwell/mainloop_tma_ws.h",
    "sage_attention/blackwell/named_barrier.h",
    "sage_attention/blackwell/params.h",
    "sage_attention/blackwell/softmax_fused.h",
    "sage_attention/blackwell/static_switch.h",
    "sage_attention/blackwell/tile_scheduler.h",
    "sage_attention/blackwell/utils.h",
]

# SM100 (Blackwell) FP4 quantization kernel
[kernel._fp4quant_sm100]
backend = "cuda"
cuda-capabilities = ["10.0a"]
cuda-minver = "12.8"
cuda-flags = [
    "-O3",
    "-std=c++17",
    "-U__CUDA_NO_HALF_OPERATORS__",
    "-U__CUDA_NO_HALF_CONVERSIONS__",
    "-U__CUDA_NO_BFLOAT16_OPERATORS__",
    "-U__CUDA_NO_BFLOAT16_CONVERSIONS__",
    "-U__CUDA_NO_BFLOAT162_OPERATORS__",
    "-U__CUDA_NO_BFLOAT162_CONVERSIONS__",
    "--expt-relaxed-constexpr",
    "--expt-extended-lambda",
    "--use_fast_math",
    "-DNDEBUG",
    "-DEXECMODE=0",
]
cxx-flags = [
    "-O3",
    "-std=c++17",
]
depends = ["torch"]
include = [
    "sage_attention/quantization",
]
src = [
    "sage_attention/quantization/fp4_quantization_4d.cu",
    "sage_attention/quantization/cuda_utils.h",
]
